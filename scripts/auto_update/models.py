# NautilusTrader Auto-Update Pipeline - Pydantic Models
#
# Data models for changelog parsing, impact analysis, and update operations.

"""Pydantic models for NautilusTrader Auto-Update Pipeline."""

from __future__ import annotations

from datetime import UTC, datetime
from enum import Enum
from pathlib import Path
from typing import Any

from pydantic import BaseModel, ConfigDict, Field, field_validator, model_validator


# =============================================================================
# Core Enums (T004)
# =============================================================================


class Severity(str, Enum):
    """Severity of a breaking change.

    - CRITICAL: Compilation breaks, import errors - blocks all auto-update
    - HIGH: Tests fail, runtime errors - requires Claude dispatch
    - MEDIUM: Deprecation warnings, API changes - may auto-fix
    - LOW: Minor signature changes, type hints - safe to auto-update
    """

    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


class Recommendation(str, Enum):
    """Action recommendation based on impact analysis.

    - AUTO: Safe to auto-update (confidence >= 85)
    - DELAYED: Auto-merge after 24h review window (confidence 65-84)
    - MANUAL: Requires human review (confidence < 65)
    - BLOCKED: Critical breaking changes, do not proceed
    """

    AUTO = "auto"
    DELAYED = "delayed"
    MANUAL = "manual"
    BLOCKED = "blocked"


class ConfidenceLevel(str, Enum):
    """Confidence level for update safety.

    - VERY_HIGH: >= 85 - Auto-merge with 2h safety delay
    - HIGH: 65-84 - Auto-merge after 24h + optional review
    - NEUTRAL: 40-64 - Manual review required
    - LOW: < 40 - Manual review + ping, email fallback
    """

    VERY_HIGH = "very_high"
    HIGH = "high"
    NEUTRAL = "neutral"
    LOW = "low"


# =============================================================================
# Input Models - from changelog.json (T005)
# =============================================================================


class OpenIssue(BaseModel):
    """GitHub issue from changelog."""

    number: int
    title: str
    state: str = "open"
    labels: list[str] = Field(default_factory=list)
    created_at: datetime | None = None
    updated_at: datetime | None = None
    author: str = ""
    url: str = ""
    comments: int = 0


class ChangelogData(BaseModel):
    """Raw changelog data from N8N-generated JSON.

    Source: docs/nautilus/nautilus-trader-changelog.json
    """

    timestamp: datetime
    stable_version: str
    nightly_commits: int = 0
    breaking_changes: list[str] = Field(default_factory=list)
    open_issues: dict[str, Any] = Field(default_factory=dict)

    @property
    def has_breaking_changes(self) -> bool:
        """Check if there are any breaking changes."""
        return len(self.breaking_changes) > 0

    @property
    def bug_list(self) -> list[dict[str, Any]]:
        """Extract bug list from open_issues."""
        return self.open_issues.get("bug_list", [])

    @property
    def feature_list(self) -> list[dict[str, Any]]:
        """Extract feature list from open_issues."""
        return self.open_issues.get("feature_list", [])


# =============================================================================
# Analysis Models (T006)
# =============================================================================


class BreakingChange(BaseModel):
    """Parsed breaking change with impact analysis.

    Extracted from ChangelogData.breaking_changes strings.
    """

    description: str = Field(description="Human-readable description of the change")
    affected_pattern: str = Field(
        default="", description="Regex pattern to grep in codebase (e.g., 'def on_tick\\(')"
    )
    severity: Severity = Severity.MEDIUM
    migration_guide: str | None = Field(
        default=None, description="How to migrate (e.g., 'Replace on_tick with on_quote_tick')"
    )

    @field_validator("affected_pattern", mode="before")
    @classmethod
    def default_pattern_from_description(cls, v: str, info) -> str:
        """Generate pattern from description if not provided."""
        if v:
            return v
        # Will be populated by analyzer
        return ""


class AffectedFile(BaseModel):
    """File impacted by a breaking change.

    Generated by analyzer.grep_codebase().
    """

    model_config = ConfigDict(arbitrary_types_allowed=True)

    path: Path
    line_numbers: list[int] = Field(default_factory=list)
    breaking_change: BreakingChange
    can_auto_fix: bool = False
    fix_type: str | None = Field(
        default=None, description="Type of fix: 'import_rename' | 'method_rename' | None"
    )


# =============================================================================
# Impact Report Model (T007)
# =============================================================================


class ImpactReport(BaseModel):
    """Complete impact analysis for a version update.

    Core output of the analyzer module, consumed by updater and dispatcher.
    """

    version: str = Field(description="Target version (e.g., '1.222.0')")
    previous_version: str | None = Field(default=None, description="Current version before update")
    breaking_changes: list[BreakingChange] = Field(default_factory=list)
    affected_files: list[AffectedFile] = Field(default_factory=list)
    total_affected_lines: int = 0
    confidence_score: float = Field(
        default=100.0, ge=0.0, le=100.0, description="Confidence score for safe update (0-100)"
    )
    confidence_level: ConfidenceLevel = ConfidenceLevel.VERY_HIGH
    can_auto_update: bool = True
    recommendation: Recommendation = Recommendation.AUTO
    generated_at: datetime = Field(default_factory=lambda: datetime.now(UTC))

    @model_validator(mode="after")
    def validate_confidence_consistency(self) -> "ImpactReport":
        """Ensure confidence_level matches score, and recommendation is consistent."""
        # Calculate expected confidence level from score
        if self.confidence_score >= 85:
            expected_level = ConfidenceLevel.VERY_HIGH
        elif self.confidence_score >= 65:
            expected_level = ConfidenceLevel.HIGH
        elif self.confidence_score >= 40:
            expected_level = ConfidenceLevel.NEUTRAL
        else:
            expected_level = ConfidenceLevel.LOW

        # Auto-correct if needed
        self.confidence_level = expected_level

        # Check for CRITICAL breaking changes
        has_critical = any(bc.severity == Severity.CRITICAL for bc in self.breaking_changes)

        # Set recommendation based on confidence and criticality
        if has_critical:
            self.recommendation = Recommendation.BLOCKED
            self.can_auto_update = False
        elif expected_level == ConfidenceLevel.VERY_HIGH:
            self.recommendation = Recommendation.AUTO
            self.can_auto_update = True
        elif expected_level == ConfidenceLevel.HIGH:
            self.recommendation = Recommendation.DELAYED
            self.can_auto_update = True
        else:
            self.recommendation = Recommendation.MANUAL
            self.can_auto_update = False

        return self

    def should_dispatch_claude(self) -> bool:
        """Check if Claude Code should handle this update.

        Returns True if:
        - Recommendation is MANUAL (needs complex fixes)
        - No CRITICAL severity breaking changes (would block entirely)
        """
        return self.recommendation == Recommendation.MANUAL and not any(
            bc.severity == Severity.CRITICAL for bc in self.breaking_changes
        )


# =============================================================================
# Operation Models (T008)
# =============================================================================


class TestResult(BaseModel):
    """Result of running test suite via pytest."""

    passed: bool = False
    total_tests: int = 0
    failed_tests: int = 0
    skipped_tests: int = 0
    failed_test_names: list[str] = Field(default_factory=list)
    duration_seconds: float = 0.0
    coverage_percent: float | None = None

    @property
    def passed_tests(self) -> int:
        """Calculate passed tests from total - failed - skipped."""
        return self.total_tests - self.failed_tests - self.skipped_tests


class UpdateResult(BaseModel):
    """Result of auto-update operation.

    Generated by updater.auto_update().
    """

    success: bool = False
    version: str = ""
    branch_name: str = ""
    pr_url: str | None = None
    test_result: TestResult | None = None
    error_message: str | None = None
    created_at: datetime = Field(default_factory=lambda: datetime.now(UTC))

    @model_validator(mode="after")
    def validate_result_consistency(self) -> "UpdateResult":
        """Ensure result fields are consistent with success status."""
        if self.success:
            if not self.branch_name:
                raise ValueError("branch_name required when success=True")
        else:
            if not self.error_message:
                # Allow success=False without error_message for tests-failed case
                pass
        return self


# =============================================================================
# Dispatch Model (T009)
# =============================================================================


class DispatchResult(BaseModel):
    """Result of Claude Code dispatch.

    Generated by dispatcher.dispatch_claude_code().
    """

    dispatched: bool = False
    task_prompt: str = ""
    agent_id: str | None = None
    pr_url: str | None = None
    error_message: str | None = None
    timeout_reached: bool = False


# =============================================================================
# Configuration Model (T010)
# =============================================================================


class AutoUpdateConfig(BaseModel):
    """Configuration for auto-update pipeline.

    Can be loaded from config.toml or environment variables.
    """

    model_config = ConfigDict(arbitrary_types_allowed=True)

    # Paths
    changelog_path: Path = Path("docs/nautilus/nautilus-trader-changelog.json")
    source_dirs: list[Path] = Field(
        default_factory=lambda: [
            Path("strategies"),
            Path("scripts"),
            Path("tests"),
        ]
    )
    pyproject_path: Path = Path("pyproject.toml")

    # Git settings
    branch_prefix: str = "update/v"
    remote: str = "origin"
    base_branch: str = "master"

    # Confidence thresholds
    auto_merge_threshold: int = Field(
        default=85, ge=0, le=100, description="Confidence score for auto-merge"
    )
    delayed_merge_threshold: int = Field(
        default=65, ge=0, le=100, description="Confidence score for delayed merge"
    )

    # Timeouts
    test_timeout_seconds: int = Field(default=600, ge=60)  # 10 minutes
    claude_timeout_seconds: int = Field(default=1800, ge=300)  # 30 minutes

    # Behavior
    dry_run: bool = False
    skip_tests: bool = False

    # Notifications
    discord_webhook_url: str | None = None
    email_recipient: str | None = None
